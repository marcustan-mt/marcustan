{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install bs4\n",
    "! pip install fuzzywuzzy\n",
    "! pip install geohash\n",
    "! pip install python-geohash\n",
    "! pip install pygeohash\n",
    "! pip install python-Levenshtein\n",
    "! pip install proximityhash\n",
    "import urllib.request\n",
    "import requests\n",
    "import numpy as np\n",
    "from urllib.request import Request, urlopen\n",
    "import pandas as pd\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from random import randint\n",
    "import time\n",
    "import math\n",
    "import csv\n",
    "import string\n",
    "# import json\n",
    "# from selenium import webdriver\n",
    "# from selenium.webdriver.support.ui import Select \n",
    "# from selenium.webdriver.support.ui import WebDriverWait\n",
    "# from selenium.webdriver.support import expected_conditions as EC\n",
    "# from selenium.webdriver.common.by import By\n",
    "# from selenium.webdriver.common.action_chains import ActionChains\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "# import Geohash\n",
    "# import pygeohash as pgh\n",
    "# import proximityhash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parsing webpage\n",
    "url_front = \"https://giantonline.com.sg/\"\n",
    "req = Request(url_front, headers={'User-Agent': 'Chrome//5.0'})\n",
    "page = urlopen(req)\n",
    "soup = BeautifulSoup(page)\n",
    "all_left_menu_item = soup.find('div', {'class' : 'left-menu'})\n",
    "\n",
    "\n",
    "#Converting cateogries into string for regex & to generate the broad categories\n",
    "broad_categories = all_left_menu_item.findAll('a', {'class' : 'department'})\n",
    "for i in range(0,len(broad_categories)):\n",
    "    broad_categories[i] = str(broad_categories[i])\n",
    "    broad_categories[i] = re.findall(r'\">(.*?)</a>', broad_categories[i])[0]\n",
    "    broad_categories[i] = broad_categories[i].replace(\", \", \"-\")\n",
    "    broad_categories[i] = broad_categories[i].replace(\" &amp; \", \"-\")\n",
    "    if broad_categories[i][-1] == \" \":\n",
    "        broad_categories[i] = broad_categories[i][0:-1]\n",
    "    broad_categories[i] = broad_categories[i].replace(\" \", \"-\")\n",
    "\n",
    "#Cos giant fked-up, name their product differently\n",
    "broad_categories[1] = 'Beers-Wines-Spirits'\n",
    "\n",
    "#Converting broad categories into smaller categories. For e.g. 'Rice' is under 'Food Pantry'    \n",
    "narrow_categories = all_left_menu_item.findAll('li', {'class' : 'dropdown category'})\n",
    "narrow_categories = narrow_categories[1:]\n",
    "for i in range(0,len(narrow_categories)):\n",
    "    narrow_categories[i] = str(narrow_categories[i])\n",
    "    narrow_categories[i] = re.findall(r'\">(.*?)</a>', narrow_categories[i])[1:]\n",
    "    for x in range(0,len(narrow_categories[i])):\n",
    "        narrow_categories[i][x] = narrow_categories[i][x].replace(\", \", \"-\")\n",
    "        narrow_categories[i][x] = narrow_categories[i][x].replace(\" &amp; \", \"-\")\n",
    "        narrow_categories[i][x] = narrow_categories[i][x].replace(\" \", \"-\")\n",
    "        narrow_categories[i][x] = narrow_categories[i][x].replace(\".\", \"\")\n",
    "        narrow_categories[i][x] = narrow_categories[i][x].replace(\"'\", \"\")\n",
    "\n",
    "#Again cos giant fked-up, name their products differently\n",
    "narrow_categories[0][-1] = 'international-groceries'\n",
    "narrow_categories[2][-1] = 'chocolate-malts-adult-milk'\n",
    "\n",
    "# Combining the list of URLs to form the webpage for only the first page        \n",
    "combine_categories = list()\n",
    "for i in range(0,len(broad_categories)):\n",
    "    for x in range(0, len(narrow_categories[i])):\n",
    "        word = str('https://giantonline.com.sg/' + str(broad_categories[i]) + '/' + str(narrow_categories[i][x]) + '/')\n",
    "        combine_categories.append(word)\n",
    "\n",
    "# Coming up with the full list of product with page\n",
    "combine_cat_page = list()\n",
    "for x in range(0,len(combine_categories)):\n",
    "    for i in range(0,30):\n",
    "        webpagelink = combine_categories[x] + '?Product_page=' + str(i)\n",
    "        combine_cat_page.append(webpagelink)\n",
    "\n",
    "\n",
    "\n",
    "print(len(combine_cat_page))\n",
    "# print(all_left_menu_item)\n",
    "# len(narrow_categories)\n",
    "# print(narrow_categories[1])\n",
    "# broad_categories\n",
    "print(combine_cat_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Conducting actual webscraping\n",
    "product_details = {}\n",
    "df = pd.DataFrame()\n",
    "for i in range(0,len(combine_cat_page)):\n",
    "    req = Request(combine_cat_page[i], headers={'User-Agent': 'Chrome//5.0'})\n",
    "    page = urlopen(req)\n",
    "    soup = BeautifulSoup(page)\n",
    "    items = soup.findAll('li', {'class' : 'col-xs-6 col-sm-4 col-md-3 col-lg-2 open-product-detail algolia-click'})\n",
    "    current_page = items[0].find('a', {'class' : \"product-link\"}).get_text()\n",
    "    if i == 0 or current_page != previous_page:\n",
    "        for x in range(0,len(items)):\n",
    "            try:\n",
    "                #image source\n",
    "                src = str(items[x].find('img'))\n",
    "                product_details['Image'] = re.findall(r'src=\"(.*?)\"', src)[0]\n",
    "            except:\n",
    "                src = \"No Image Link\"\n",
    "                product_details['Image'] = src\n",
    "            try:\n",
    "                #brand name\n",
    "                brand = items[x].find('a', {'class' : 'to-brand-page'}).get_text()\n",
    "                product_details['Brand'] = brand\n",
    "            except:\n",
    "                brand = 'Brandless'\n",
    "                product_details['Brand'] = brand\n",
    "            try:\n",
    "                #further product info\n",
    "                details = items[x].find('a', {'class' : \"product-link\"}).get_text()\n",
    "                product_details['Further Details'] = details\n",
    "            except:\n",
    "                details = 'No Further Details'\n",
    "                product_details['Further Details'] = details\n",
    "            try:\n",
    "                #weight of product\n",
    "                size = items[x].find('span', {'class' : 'size'}).get_text()\n",
    "                product_details['Size'] = size\n",
    "            except:\n",
    "                size = 'No Size'\n",
    "                product_details['Size'] = size\n",
    "            try:\n",
    "                #any discounts\n",
    "                discount = items[x].find('span', {'class' : 'product-discount-label'}).get_text()\n",
    "                product_details['Discount'] = discount\n",
    "            except:\n",
    "                discount = 'No Discount'\n",
    "                product_details['Discount'] = discount\n",
    "            try:\n",
    "                #Happens when there's a discount\n",
    "                items_x = str(items[x])\n",
    "                new_price = re.findall(r'<div class=\"price product-price red\" data-price=\"(.*?)\">', items_x)[0]\n",
    "                old_price = re.findall(r'<span class=\"price old-price\">(.*?)</span>', items_x)[0]\n",
    "                product_details['Current Price'] = new_price\n",
    "                product_details['Old Price'] = old_price\n",
    "            except:\n",
    "                try:\n",
    "                    new_price = items[x].find('div', {'class' : 'price product-price'}).get_text()\n",
    "                    product_details['Current Price'] = new_price\n",
    "                    product_details['Old Price'] = 'No Discount'\n",
    "                except:\n",
    "                    new_price = items[x].find('div', {'class' : 'content_price'}).get_text().rstrip().lstrip()\n",
    "                    product_details['Current Price'] = new_price\n",
    "                    product_details['Old Price'] = 'No Discount'\n",
    "            try:\n",
    "                #link to goods under the brand\n",
    "                brand_goods = str(items[x].find('a', {'class' : 'to-brand-page'}))\n",
    "                brand_goods = re.findall(r'href=\"(.*?)\">', brand_goods)[0]\n",
    "                product_details['Brands Goods'] = brand_goods\n",
    "            except:\n",
    "                product_details['Brands Goods'] = 'No Page for Brand Goods'\n",
    "            try:\n",
    "                # Links to more details\n",
    "                more_details = str(items[x])\n",
    "                more_details = re.findall(r'<a class=\"product-link\" href=\"(.*?)\">', more_details)[0]\n",
    "                more_details = 'https://giantonline.com.sg' + more_details\n",
    "                product_details['Link to More Details'] = more_details\n",
    "            except:\n",
    "                product_details['Link to More Details'] = 'No Link'\n",
    "            try:\n",
    "                outstock = items[13].find('div', {'class' : 'btn add-cart btn-default out-of-stock'}).get_text().rstrip().lstrip()\n",
    "                product_details['Stock'] = outstock\n",
    "            except:\n",
    "                product_details['Stock'] = \"In Stock\"\n",
    "            try:\n",
    "                comb_info = combine_cat_page[0].split('_page=')\n",
    "                page_number = comb_info[0]\n",
    "                comb_cat = re.findall(r'https://giantonline.com.sg/(.*?)/?Product', comb_info[0])\n",
    "                broad_cat = comb_cat[0].split('/')[0]\n",
    "                narrow_cat = comb_cat[0].split('/')[1]\n",
    "                product_details['Page Number'] = page_number\n",
    "                product_details['Broad Category'] = broad_cat\n",
    "                product_details['Narrow Category'] = narrow_cat\n",
    "            except:\n",
    "                product_details['Page Number'] = 'No Page'\n",
    "                product_details['Broad Category'] = 'No Broad Cat'\n",
    "                product_details['Narrow Category'] = 'No Narrow Cat'\n",
    "            df = df.append(product_details, ignore_index = True)\n",
    "        previous_page = current_page     \n",
    "    print(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df)\n",
    "df.to_csv('Giant Supermarket Scrape.csv', sep=',')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
